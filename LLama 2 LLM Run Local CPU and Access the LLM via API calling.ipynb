{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeJXiCJMyhbWj9tbk8sU6Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikeyanrathinam/Large-Language-Models-LLM/blob/main/LLama_2_LLM_Run_Local_CPU_and_Access_the_LLM_via_API_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)![Visual Studio Code](https://img.shields.io/badge/Visual%20Studio%20Code-0078d7.svg?style=for-the-badge&logo=visual-studio-code&logoColor=white)![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white)![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)![Kali](https://img.shields.io/badge/Kali-268BEE?style=for-the-badge&logo=kalilinux&logoColor=white)![Postman](https://img.shields.io/badge/Postman-FF6C37?style=for-the-badge&logo=postman&logoColor=white)![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)![Google](https://img.shields.io/badge/google-4285F4?style=for-the-badge&logo=google&logoColor=white)![DuckDuckGo](https://img.shields.io/badge/DuckDuckGo-DE5833?style=for-the-badge&logo=DuckDuckGo&logoColor=white)![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)![Windows 11](https://img.shields.io/badge/Windows%2011-%230079d5.svg?style=for-the-badge&logo=Windows%2011&logoColor=white)\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-181717.svg?style=for-the-badge&logo=GitHub&logoColor=white)](https://github.com/karthikeyanrathinam/)\n",
        "[![Linkedin](https://img.shields.io/badge/LinkedIn-0A66C2.svg?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://www.linkedin.com/in/karthikeyanrathinam/)\n",
        "[![YouTube](https://img.shields.io/badge/YouTube-FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/@linkagethink)\n",
        "[![Gmail](https://img.shields.io/badge/Gmail-EA4335.svg?style=for-the-badge&logo=Gmail&logoColor=white)](mailto:karthikeyanr1801@gmail.com)\n",
        "\n",
        "# **LLama 2 LLM Run Local CPU and Access the LLM via API calling**\n",
        "This project implements a simple server-client architecture to interact with a language model using HTTP requests.\n",
        "\n",
        "# Table of Contents\n",
        "- [What](#what)\n",
        "- [Overview](#Overview)\n",
        "- [Installation](#Installation)\n",
        "- [Usage](#Usage)\n",
        "- [Server Code](#ServerCode)\n",
        "- [Client Code](#ClientCode)\n",
        "- [Contributing](#Contributing)\n",
        "- [License](#License)\n",
        "\n",
        "## Overview\n",
        "This project utilizes a language model to generate responses to user queries. The ```server-script``` handles HTTP requests and uses the language model to generate responses, while the ```client-script``` interacts with the server to retrieve these responses.\n",
        "\n",
        "## Installation\n",
        "Requirements:\n",
        "\n",
        "Python 3.x\n",
        "Dependencies: ```ctransformers, langchain```\n",
        "\n",
        "Installation Steps:\n",
        "\n",
        "Clone this repository.\n",
        "Install dependencies:\n",
        "```python\n",
        "pip install ctransformers langchain\n",
        "```\n",
        "## Usage\n",
        "To use the provided scripts:\n",
        "\n",
        "#Server Side\n",
        "Run the server script ```serverScript.py```:\n",
        "\n",
        "```bash\n",
        "python serverScript.py\n",
        "```\n",
        "The server will start on port 8000.\n",
        "\n",
        "#Client Side\n",
        "Use the client script ```clientScript.py``` to interact with the server:\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "```\n",
        "\n",
        "```python\n",
        "url = 'http://localhost'  # Replace with your server address if needed\n",
        "Question = 'Hello, server!'\n",
        "port = 8000\n",
        "result = api_calls(url, Question, port)\n",
        "print(result)\n",
        "```\n",
        "## ServerCode\n",
        "The serverScript.py file includes functionality to handle GET requests and generate model responses:\n",
        "\n",
        "```python\n",
        "# Generate model response\n",
        "model_response=self.model_response(received_question)\n",
        "\n",
        "# Prepare JSON response\n",
        "response_data = {'response': f\"{model_response}\"}\n",
        "response = json.dumps(response_data).encode('utf-8')\n",
        "```\n",
        "\n",
        "## ClientCode\n",
        "\n",
        "The clientScript.py file makes API calls to the server and handles responses:\n",
        "\n",
        "```python\n",
        "response = requests.get(f\"{url}:{port}?question={Question}\")\n",
        "```\n",
        "## Contributing\n",
        "Contributions to this project are welcome! If you'd like to contribute, please open an issue or submit a pull request.\n",
        "\n",
        "## License\n",
        "This project is licensed under the MIT License.\n",
        "\n",
        "## Follow\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-181717.svg?style=for-the-badge&logo=GitHub&logoColor=white)](https://github.com/karthikeyanrathinam/)\n",
        "[![Linkedin](https://img.shields.io/badge/LinkedIn-0A66C2.svg?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://www.linkedin.com/in/karthikeyanrathinam/)\n",
        "[![YouTube](https://img.shields.io/badge/YouTube-FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/@linkagethink)\n",
        "[![Gmail](https://img.shields.io/badge/Gmail-EA4335.svg?style=for-the-badge&logo=Gmail&logoColor=white)](mailto:karthikeyanr1801@gmail.com)\n"
      ],
      "metadata": {
        "id": "5p-FloE5dVMo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8eXnarudTLu"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements.txt\n",
        "ctransformers\n",
        "langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "T1yApBOOdrwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile serverScript.py\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='llama_General_Chabot.log',  # You can adjust the log file name here\n",
        "    filemode='a',\n",
        "    format='[%(asctime)s] [%(levelname)s] [%(filename)s] [%(lineno)s:%(funcName)s()] %(message)s',\n",
        "    datefmt='%Y-%b-%d %H:%M:%S'\n",
        ")\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "log_level_env = 'INFO'  # You can adjust the log level here\n",
        "log_level_dict = {\n",
        "    'DEBUG': logging.DEBUG,\n",
        "    'INFO': logging.INFO,\n",
        "    'WARNING': logging.WARNING,\n",
        "    'ERROR': logging.ERROR,\n",
        "    'CRITICAL': logging.CRITICAL\n",
        "}\n",
        "if log_level_env in log_level_dict:\n",
        "    log_level = log_level_dict[log_level_env]\n",
        "else:\n",
        "    log_level = log_level_dict['INFO']\n",
        "LOGGER.setLevel(log_level)\n",
        "\n",
        "llm=\"\"\n",
        "class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):\n",
        "    def _set_headers(self, content_type='text/html', status=200):\n",
        "\n",
        "        \"\"\"\n",
        "        Set the HTTP headers for the response.\n",
        "\n",
        "        Parameters:\n",
        "        - content_type (str): The content type of the response.\n",
        "        - status (int): The HTTP status code.\n",
        "        \"\"\"\n",
        "        try:\n",
        "          self.send_response(status)\n",
        "          self.send_header('Content-type', content_type)\n",
        "          self.end_headers()\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "    def model_response(self, question):\n",
        "\n",
        "        \"\"\"\n",
        "        Get the response from the language model based on the provided question.\n",
        "\n",
        "        Parameters:\n",
        "        - question (str): The input question for the language model.\n",
        "\n",
        "        Returns:\n",
        "        - str: The response generated by the language model.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "          LOGGER.info(\"Getting Model Response...\")\n",
        "          global llm\n",
        "\n",
        "          # Check if the language model is not initialized\n",
        "          if llm == \"\":\n",
        "\n",
        "              # Initialize the CTransformers language mode\n",
        "              llm = CTransformers(model=\"TheBloke/Llama-2-7B-Chat-GGML\", model_file = 'llama-2-7b-chat.ggmlv3.q2_K.bin', callbacks=[StreamingStdOutCallbackHandler()])\n",
        "\n",
        "          # Template for language model input\n",
        "          template = \"\"\"\n",
        "          [INST] <<SYS>>\n",
        "          You are a helpful, respectful and honest assistant. Your answers are always brief.\n",
        "          <</SYS>>\n",
        "          {text}[/INST]\n",
        "          \"\"\"\n",
        "\n",
        "          # Create a prompt template with input variables\n",
        "          prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "\n",
        "          # Create an LLMChain with the defined prompt and language model\n",
        "          llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "          # Generate a response using the LLMChain\n",
        "          response = llm_chain.run(question)\n",
        "\n",
        "          return response\n",
        "\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "    def do_GET(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Handle GET requests by extracting the question from the query parameters\n",
        "        and responding with the generated model response in JSON format.\n",
        "        \"\"\"\n",
        "        try:\n",
        "          parsed_url = urlparse(self.path)\n",
        "          query_params = parse_qs(parsed_url.query)\n",
        "\n",
        "          # Get the question from query parameters\n",
        "          received_question = query_params.get('question', [''])[0]\n",
        "\n",
        "          # Generate model response\n",
        "          model_response=self.model_response(received_question)\n",
        "\n",
        "          # Prepare JSON response\n",
        "          response_data = {'response': f\"{model_response}\"}\n",
        "          response = json.dumps(response_data).encode('utf-8')\n",
        "\n",
        "          # Set headers and write the response\n",
        "          self._set_headers('application/json')\n",
        "          self.wfile.write(response)\n",
        "        except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "def run(server_class=HTTPServer, handler_class=SimpleHTTPRequestHandler, port=8000):\n",
        "\n",
        "    \"\"\"\n",
        "    Run the HTTP server with the specified server class, request handler class, and port.\n",
        "\n",
        "    Parameters:\n",
        "    - server_class: The HTTP server class.\n",
        "    - handler_class: The request handler class.\n",
        "    - port: The port on which the server will listen.\n",
        "    \"\"\"\n",
        "    try:\n",
        "      server_address = ('', port)\n",
        "      httpd = server_class(server_address, handler_class)\n",
        "      print(f\"Server running on port {port}\")\n",
        "      httpd.serve_forever()\n",
        "    except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "metadata": {
        "id": "Irkasr6iduYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run Server Script in user local machine."
      ],
      "metadata": {
        "id": "ZHEJQI4LecTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile clientScript.py\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='api_call.log',  # You can adjust the log file name here\n",
        "    filemode='a',\n",
        "    format='[%(asctime)s] [%(levelname)s] [%(filename)s] [%(lineno)s:%(funcName)s()] %(message)s',\n",
        "    datefmt='%Y-%b-%d %H:%M:%S'\n",
        ")\n",
        "LOGGER = logging.getLogger(__name__)\n",
        "\n",
        "log_level_env = 'INFO'  # You can adjust the log level here\n",
        "log_level_dict = {\n",
        "    'DEBUG': logging.DEBUG,\n",
        "    'INFO': logging.INFO,\n",
        "    'WARNING': logging.WARNING,\n",
        "    'ERROR': logging.ERROR,\n",
        "    'CRITICAL': logging.CRITICAL\n",
        "}\n",
        "if log_level_env in log_level_dict:\n",
        "    log_level = log_level_dict[log_level_env]\n",
        "else:\n",
        "    log_level = log_level_dict['INFO']\n",
        "LOGGER.setLevel(log_level)\n",
        "\n",
        "def api_calls(url,Question,port):\n",
        "\n",
        "    \"\"\"\n",
        "    Make an API call to the specified URL and port, passing the given question as a query parameter.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The base URL of the server.\n",
        "    - question (str): The question to be sent as a query parameter.\n",
        "    - port (int): The port on which the server is running.\n",
        "\n",
        "    Returns:\n",
        "    - dict or str: The JSON response from the server if successful, or an error message with the status code.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "      LOGGER.info(\"Your Response Generating...\")\n",
        "      # Make a GET request to the server with the question as a query parameter\n",
        "      response = requests.get(f\"{url}:{port}?question={Question}\")\n",
        "\n",
        "      # Check if the request was successful (status code 200)\n",
        "      if response.status_code == 200:\n",
        "          # Return the JSON response\n",
        "          return response.json()\n",
        "      else:\n",
        "          # Return an error message with the status code\n",
        "          return \"Error:\", response.status_code\n",
        "    except Exception as e:\n",
        "            return str(e)\n",
        "\n",
        "url = 'http://localhost'  # Replace with your server address if needed\n",
        "Question = 'Hello, server!'\n",
        "port = 8000\n",
        "result = api_calls(url,Question,port)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "h_kvY3yEdwOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python clientScript.py"
      ],
      "metadata": {
        "id": "KNFkbmyCdyU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
