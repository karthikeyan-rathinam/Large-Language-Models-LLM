{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karthikeyanrathinam/Large-Language-Models-LLM/blob/main/Mistral-7B-Instruct1-Multi-PDF-Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Mistral-7B-Instruct1-Multi-PDF-Chatbot**\n",
        "\n",
        "![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)![OpenAI](https://img.shields.io/badge/OpenAI-412991.svg?style=for-the-badge&logo=OpenAI&logoColor=white)![Jupyter Notebook](https://img.shields.io/badge/jupyter-%23FA0F00.svg?style=for-the-badge&logo=jupyter&logoColor=white)![Visual Studio Code](https://img.shields.io/badge/Visual%20Studio%20Code-0078d7.svg?style=for-the-badge&logo=visual-studio-code&logoColor=white)![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white)![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)![Kali](https://img.shields.io/badge/Kali-268BEE?style=for-the-badge&logo=kalilinux&logoColor=white)![Postman](https://img.shields.io/badge/Postman-FF6C37?style=for-the-badge&logo=postman&logoColor=white)![Git](https://img.shields.io/badge/git-%23F05033.svg?style=for-the-badge&logo=git&logoColor=white)![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)![Google](https://img.shields.io/badge/google-4285F4?style=for-the-badge&logo=google&logoColor=white)![DuckDuckGo](https://img.shields.io/badge/DuckDuckGo-DE5833?style=for-the-badge&logo=DuckDuckGo&logoColor=white)![Edge](https://img.shields.io/badge/Microsoft%20Edge-0078D7.svg?style=for-the-badge&logo=Microsoft-Edge&logoColor=white)![Windows](https://img.shields.io/badge/Windows-0078D6?style=for-the-badge&logo=windows&logoColor=white)![Windows 11](https://img.shields.io/badge/Windows%2011-%230079d5.svg?style=for-the-badge&logo=Windows%2011&logoColor=white)![Open Project](https://img.shields.io/badge/OpenProject-0770B8.svg?style=for-the-badge&logo=OpenProject&logoColor=white)![Open Access](https://img.shields.io/badge/Open%20Access-F68212.svg?style=for-the-badge&logo=Open-Access&logoColor=white)\n",
        "\n",
        "# **Let's connect :**\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-181717.svg?style=for-the-badge&logo=GitHub&logoColor=white)](https://github.com/karthikeyanrathinam/)\n",
        "[![Linkedin](https://img.shields.io/badge/LinkedIn-0A66C2.svg?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://www.linkedin.com/in/karthikeyanrathinam/)\n",
        "[![YouTube](https://img.shields.io/badge/YouTube-FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/@linkagethink)\n",
        "[![Gmail](https://img.shields.io/badge/Gmail-EA4335.svg?style=for-the-badge&logo=Gmail&logoColor=white)](mailto:karthikeyanr1801@gmail.com)\n",
        "\n",
        "# QA Processor\n",
        "\n",
        "This Python script is designed to process user input queries and provide answers using a retrieval-based question answering system. It utilizes LangChain library components such as document loaders, text splitters, embeddings, vector stores, and llms for language modeling.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1. Clone the repository:\n",
        "\n",
        "git clone https://github.com/karthikeyanrathinam/Mistral-7B-Multi-Document-Chatbot.git\n",
        "\n",
        "\n",
        "2. Install the required dependencies:\n",
        "```python\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Ensure you have a directory containing PDF documents from which the QA system will retrieve information.\n",
        "\n",
        "2. Download the pre-trained model and save it to a local directory.\n",
        "\n",
        "3. Modify the `pdf_directory` and `model_path` variables in the script according to your local file paths.\n",
        "\n",
        "4. Run the script:\n",
        "\n",
        "\n",
        "## python app.py\n",
        "\n",
        "\n",
        "5. Input your query at the prompt. Type 'exit' to quit the program.\n",
        "\n",
        "## Class Overview\n",
        "\n",
        "### `QAProcessor`\n",
        "\n",
        "- `__init__(pdf_directory, model_path)`: Initializes the QAProcessor class with necessary components such as document loader, text splitter, embeddings, vector store, and language model.\n",
        "\n",
        "- `initialize_qa()`: Initializes the retrieval-based question answering system.\n",
        "\n",
        "- `process_input(user_input)`: Processes user input queries and provides answers using the initialized QA system.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.6+\n",
        "- LangChain library\n",
        "- PDF documents for retrieval\n",
        "- Pre-trained language model\n",
        "\n",
        "## Example\n",
        "```python\n",
        "from qa_processor import QAProcessor\n",
        "\n",
        "pdf_directory = \"/path/to/pdf_directory/\"\n",
        "model_path = \"/path/to/pretrained_model/\"\n",
        "\n",
        "processor = QAProcessor(pdf_directory, model_path)\n",
        "\n",
        "while True:\n",
        "    user_input = input(f\"Input Prompt: \")\n",
        "    processor.process_input(user_input)\n",
        "```\n",
        "\n",
        "## Contributing\n",
        "Contributions are welcome! Please fork the repository and submit a pull request with your improvements.\n",
        "\n",
        "## License\n",
        "This project is licensed under the MIT License - see the LICENSE file for details.\n",
        "\n",
        "Replace `karthikeyanrathinam/Mistral-7B-Multi-Document-Chatbot` with your GitHub username and repository name in the installation instructions. Adjust the file paths in the usage section to match your local environment. This README provides instructions for installation, usage, class overview, requirements, an example, contributing guidelines, and licensing information for the project.\n",
        "\n",
        "\n",
        "\n",
        "# **Follow**\n",
        "# Feel free to reach out if you have any questions or need further assistance.\n",
        "\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-181717.svg?style=for-the-badge&logo=GitHub&logoColor=white)](https://github.com/karthikeyanrathinam/)\n",
        "[![Linkedin](https://img.shields.io/badge/LinkedIn-0A66C2.svg?style=for-the-badge&logo=LinkedIn&logoColor=white)](https://www.linkedin.com/in/karthikeyanrathinam/)\n",
        "[![YouTube](https://img.shields.io/badge/YouTube-FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/@linkagethink)\n",
        "[![Gmail](https://img.shields.io/badge/Gmail-EA4335.svg?style=for-the-badge&logo=Gmail&logoColor=white)](mailto:karthikeyanr1801@gmail.com)\n",
        "\n"
      ],
      "metadata": {
        "id": "OvZEIllaYOJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "langchain\n",
        "torch\n",
        "sentence_transformers\n",
        "faiss-cpu\n",
        "huggingface-hub\n",
        "pypdf\n",
        "accelerate\n",
        "llama-cpp-python\n",
        "git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "6MZPJ7ZjS5wm",
        "outputId": "789c657d-a462-43ae-a56d-398d33210d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "cFnT6xhrTUdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QNXlGKC_c3-"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9Y5Va538RDL"
      },
      "outputs": [],
      "source": [
        "#Import Model\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path=\"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    top_p=1,\n",
        "    verbose=True,\n",
        "    n_ctx=4096\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile QAProcessor.py\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "import sys\n",
        "\n",
        "class QAProcessor:\n",
        "    def __init__(self, pdf_directory, model_path):\n",
        "        self.loader = PyPDFDirectoryLoader(pdf_directory)\n",
        "        self.data = self.loader.load()\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "        self.text_chunks = self.text_splitter.split_documents(self.data)\n",
        "        self.embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.vector_store = FAISS.from_documents(self.text_chunks, embedding=self.embeddings)\n",
        "        self.llm = LlamaCpp(\n",
        "            streaming=True,\n",
        "            model_path=model_path,\n",
        "            temperature=0.75,\n",
        "            top_p=1,\n",
        "            verbose=True,\n",
        "            n_ctx=4096\n",
        "        )\n",
        "        self.qa = self.initialize_qa()\n",
        "\n",
        "    def initialize_qa(self):\n",
        "        retriever = self.vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "        return RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    def process_input(self, user_input):\n",
        "        if user_input == 'exit':\n",
        "            print('Exiting')\n",
        "            sys.exit()\n",
        "        if user_input == '':\n",
        "            return\n",
        "        result = self.qa({'query': user_input})\n",
        "        print(f\"Answer: {result['result']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_directory = \"/content/sample_data/Data/\"\n",
        "    model_path = \"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "    processor = QAProcessor(pdf_directory, model_path)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(f\"Input Prompt: \")\n",
        "        processor.process_input(user_input)\n"
      ],
      "metadata": {
        "id": "S3OqYNP3V6xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from QAProcessor import QAProcessor, process_input\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_directory = \"/content/sample_data/Data/\"\n",
        "    model_path = \"/content/drive/MyDrive/Model/mistral-7b-instruct-v0.1.Q4_K_M.gguf\"\n",
        "\n",
        "    processor = QAProcessor(pdf_directory, model_path)\n",
        "\n",
        "    while True:\n",
        "        user_input = input(f\"Input Prompt: \")\n",
        "        processor.process_input(user_input)\n"
      ],
      "metadata": {
        "id": "fjF5qgqkXQM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python app.py"
      ],
      "metadata": {
        "id": "PulLpqMcbWtA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
